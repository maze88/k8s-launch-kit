# K8s Launch Kit - CLI for configuring NVIDIA cloud-native solutions

K8s Launch Kit (l8k) is a CLI tool for deploying and managing NVIDIA cloud-native solutions on Kubernetes. The tool helps provide flexible deployment workflows for optimal network performance with SR-IOV, RDMA, and other networking technologies.

## Operation Phases

### Discover Cluster Configuration
Deploy a minimal Network Operator profile to automatically discover your cluster's network capabilities and hardware configuration. This phase can be skipped if you provide your own configuration file.

### Select the Deployment Profile
Specify the desired deployment profile via CLI flags or with the natural language prompt for the LLM.

### Generate Deployment Files
Based on the discovered/provided configuration, generate a complete set of YAML deployment files tailored to your selected network profile.

## Installation

### Build from source

```bash
git clone <repository-url>
cd launch-kubernetes
make build
```

The binary will be available at `build/l8k`.

### Docker

Build the Docker image:

```bash
make docker-build
```

## Usage

<!-- BEGIN HELP -->
<!-- This section is automatically updated by running 'make update-readme' -->

```

K8s Launch Kit (l8k) is a CLI tool for deploying and managing NVIDIA cloud-native solutions on Kubernetes. The tool helps provide flexible deployment workflows for optimal network performance with SR-IOV, RDMA, and other networking technologies.

### Discover Cluster Configuration
Deploy a minimal Network Operator profile to automatically discover your cluster's
network capabilities and hardware configuration by using --discover-cluster-config.
This phase can be skipped if you provide your own configuration file by using --user-config.
This phase requires --kubeconfig to be specified.

### Generate Deployment Files
Based on the discovered or provided configuration, 
generate a complete set of YAML deployment files for the selected network profile. 
Files can be saved to disk using --save-deployment-files.
The profile can be defined manually with --fabric, --deployment-type and --multirail flags,
OR generated by an LLM-assisted profile generator with --prompt (requires --llm-api-key and --llm-vendor).

### Deploy to Cluster
Apply the generated deployment files to your Kubernetes cluster by using --deploy. This phase requires --kubeconfig and can be skipped if --deploy is not specified.

Usage:
  l8k [flags]
  l8k [command]

Available Commands:
  completion  Generate the autocompletion script for the specified shell
  help        Help about any command
  version     Print the version number

Flags:
      --ai                             Enable AI deployment
      --deploy                         Deploy the generated files to the Kubernetes cluster
      --deployment-type string         Select the deployment type (sriov, rdma_shared, host_device)
      --discover-cluster-config        Deploy a thin Network Operator profile to discover cluster capabilities
      --enabled-plugins string         Comma-separated list of plugins to enable (default "network-operator")
      --fabric string                  Select the fabric type to deploy (infiniband, ethernet)
  -h, --help                           help for l8k
      --kubeconfig string              Path to kubeconfig file for cluster deployment (required when using --deploy)
      --llm-api-key string             API key for the LLM API (required when using --prompt)
      --llm-api-url string             API URL for the LLM API (required when using --prompt)
      --llm-vendor string              Vendor of the LLM API (required when using --prompt) (default "openai-azure")
      --log-level string               Log level (debug, info, warn, error) (default "info")
      --multirail                      Enable multirail deployment
      --prompt string                  Path to file with a prompt to use for LLM-assisted profile generation
      --save-cluster-config string     Save discovered cluster configuration to the specified path (default "/opt/nvidia/k8s-launch-kit/cluster-config.yaml")
      --save-deployment-files string   Save generated deployment files to the specified directory (default "/opt/nvidia/k8s-launch-kit/deployment")
      --spectrum-x                     Enable Spectrum X deployment
      --user-config string             Use provided cluster configuration file instead of auto-discovery (skips cluster discovery)

Use "l8k [command] --help" for more information about a command.
```

<!-- END HELP -->

## Usage Examples

### Complete Workflow

Discover cluster config, generate files, and deploy:

```bash
l8k --discover-cluster-config --save-cluster-config ./cluster-config.yaml \
    --fabric ethernet --deployment-type sriov --multirail \
    --save-deployment-files ./deployments \
    --deploy --kubeconfig ~/.kube/config
```

### Discover Cluster Configuration

```bash
l8k --discover-cluster-config --save-cluster-config ./my-cluster-config.yaml
```

### Use Existing Configuration  

Generate and deploy with pre-existing config:

```bash
l8k --user-config ./existing-config.yaml \
    --fabric ethernet --deployment-type sriov --multirail \
    --deploy --kubeconfig ~/.kube/config
```

### Generate Deployment Files

```bash
l8k --user-config ./config.yaml \
    --fabric ethernet --deployment-type sriov --multirail \
    --save-deployment-files ./deployments
```

### Generate Deployment Files using Natural Language Prompt

```bash
echo "I want to enable multirail networking in my AI cluster" > requirements.txt
l8k --user-config ./config.yaml \
    --prompt requirements.txt --llm-vendor openai-azure --llm-api-key <OPENAI_AZURE_KEY> \
    --save-deployment-files ./deployments
```

## Docker container

You can run the l8k tool as a docker container:

```bash
docker run -v ~/launch-kubernetes/user-prompt:/user-prompt -v ~/remote-cluster/:/remote-cluster -v /tmp:/output --net=host harbor.mellanox.com/k8s-launch-kit:poc --discover-cluster-config --kubeconfig /remote-cluster/kubeconf.yaml --save-cluster-config /output/config.yaml --log-level debug  --save-deployment-files /output --fabric infiniband --deployment-type rdma_shared --multirail
```

Don't forget to enable `--net=host` and mount the necessary directories for input and output files with `-v`.

## Development

### Building

```bash
make build        # Build for current platform
make build-all    # Build for all platforms
make clean        # Clean build artifacts
```

### Testing

```bash
make test         # Run tests
make coverage     # Run tests with coverage
```

### Linting

```bash
make lint         # Run linter
make lint-check   # Install and run linter
```

### Docker

```bash
make docker-build # Build Docker image
make docker-run   # Run Docker container
```

## Contributing

1. Ensure you have Go 1.21+ installed
2. Run `make dev-setup` to install dependencies and run initial checks
3. Make your changes
4. Run `make lint` and `make test` to ensure code quality
5. Submit a pull request
